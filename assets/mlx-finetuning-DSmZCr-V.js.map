{"version":3,"file":"mlx-finetuning-DSmZCr-V.js","sources":["../../src/content/blog/mlx-finetuning.md?raw"],"sourcesContent":["export default \"---\\nid: \\\"mlx-finetuning\\\"\\ntitle: \\\"A Guide to Fine-Tuning Gemma 3 on Apple Hardware\\\"\\ndate: \\\"2025-12-29\\\"\\nreadTime: \\\"10 min read\\\"\\nexcerpt: \\\"In the current landscape of AI, bigger isn't always better. While massive models know a little bit about everything developers only need a partner that deeply understands their specific language.\\\"\\ntags: [\\\"AI\\\", \\\"Fine-tuning\\\", \\\"MLX\\\"]\\n---\\n\\nIn the current landscape of AI, bigger isn't always better. While massive models know a little bit about everything—from French poetry to Python—most developers only need a partner that deeply understands their specific language.\\n\\nThis guide shows you how to take **Gemma 3 (4B)** and turn it into a dedicated JavaScript specialist. We aren't building a world-class expert here; instead, we are showing you a **proof-of-concept** for how to customize an AI on a standard Mac using Apple’s own hardware-accelerated tools.\\n\\n---\\n\\n## 1. Organizing the Data: Teaching JavaScript Patterns\\n\\nAI models learn by seeing examples. To make a model better at JavaScript, we need to feed it a clean diet of \\\"Instruction and Response\\\" pairs. We want to filter out the \\\"noise\\\" of other programming languages so the model’s focus stays purely on JS.\\n\\n### The Preparation Script\\n\\nWe use a simple Node.js script to take raw datasets and format them into the specific \\\"language\\\" Gemma 3 understands. This script ensures every example includes the correct tags so the AI knows when you are talking and when it should respond.\\n\\n```javascript\\nconst fs = require('fs');\\nconst path = require('path');\\n\\n// This function wraps our code in tags the AI recognizes\\nconst formatGemma = (question, answer) => ({\\n    text: `<start_of_turn>user\\\\n${question}<end_of_turn>\\\\n<start_of_turn>model\\\\n${answer}<end_of_turn>`\\n});\\n\\nasync function prepareDataset() {\\n    const DATA_DIR = './data';\\n    if (!fs.existsSync(DATA_DIR)) fs.mkdirSync(DATA_DIR);\\n\\n    const rawData = JSON.parse(fs.readFileSync('dataset.json', 'utf8'));\\n    \\n    // Filter for high-quality JavaScript signals (like arrow functions)\\n    const jsData = rawData.filter(item => \\n        item.output.includes('=>') || \\n        item.output.includes('const ') || \\n        item.instruction.toLowerCase().includes('javascript')\\n    );\\n\\n    // Shuffle the data and split it (90% for training, 10% for testing)\\n    jsData.sort(() => 0.5 - Math.random());\\n    const splitIdx = Math.floor(jsData.length * 0.9);\\n\\n    const writeJsonl = (name, data) => {\\n        const stream = fs.createWriteStream(path.join(DATA_DIR, name));\\n        data.forEach(d => stream.write(JSON.stringify(formatGemma(d.instruction, d.output)) + '\\\\n'));\\n        stream.end();\\n    };\\n\\n    writeJsonl('train.jsonl', jsData.slice(0, splitIdx));\\n    writeJsonl('valid.jsonl', jsData.slice(splitIdx));\\n}\\nprepareDataset();\\n\\n```\\n\\n---\\n\\n## 2. The Training Process: MLX on the Mac\\n\\nTo do the actual \\\"teaching,\\\" we use a tool called **MLX-LM**. This was built by Apple’s researchers to make sure AI training runs as fast as possible on M1, M2, or M3/M4 chips.\\n\\nBecause training can be memory-heavy, we use a few clever settings to make sure it doesn't crash a standard laptop:\\n\\n* **Batch Size 1:** We look at one example at a time.\\n* **Grad Checkpointing:** A memory-saving trick that calculates logic only when needed.\\n\\n### The Fine-Tuning Command\\n\\nRun this in your terminal. It tells the computer to take the base Gemma model and start learning from your new data.\\n\\n```bash\\npython -m mlx_lm.lora \\\\\\n  --model google/gemma-3-4b-it \\\\\\n  --train \\\\\\n  --data ./data \\\\\\n  --batch-size 1 \\\\\\n  --num-layers 8 \\\\\\n  --iters 1000 \\\\\\n  --grad-checkpoint \\\\\\n  --adapter-path ./js_specialist_adapters\\n\\n```\\n\\n---\\n\\n## 3. Fusing the New Knowledge\\n\\nAfter training, your model has a \\\"backpack\\\" of new knowledge (the adapters). To make it easy to use in other apps, we \\\"fuse\\\" that backpack into the main model to create one single, standalone folder.\\n\\n```bash\\npython -m mlx_lm.fuse \\\\\\n  --model google/gemma-3-4b-it \\\\\\n  --adapter-path ./js_specialist_adapters \\\\\\n  --save-path ./gemma-3-js-expert\\n\\n```\\n\\n---\\n\\n## 4. Running Your Model in LM Studio\\n\\nNow that you have your own version of Gemma, you can use it in a friendly chat interface like **LM Studio**.\\n\\n1. **Find the hidden folder:** Open your terminal and type `open ~/.lmstudio/models`.\\n2. **Move your model:** Create a folder inside called `my-models`, and drop your `gemma-3-js-expert` folder in there.\\n3. **Start Chatting:** Open LM Studio. Your new JS Specialist will appear in your local library, ready to answer questions.\\n\\n---\\n\\n## Summary: A Glimpse into Custom AI\\n\\nThis project demonstrates how accessible AI customization has become. By focusing a small model on a single task, you get several benefits:\\n\\n* **Total Privacy:** Your code and your data never leave your computer.\\n* **Efficiency:** A specialized 4B model is often faster and more helpful for specific tasks than a massive, unspecialized one.\\n* **No Cloud Costs:** You are using the hardware you already own.\\n\\n**Keep in mind:** This is just a sample. To create a \\\"Production Expert\\\" that knows every library and edge case, you would need a much larger dataset (50,000+ examples) and significant compute time on high-end hardware.\\n\""],"names":["mlxFinetuning"],"mappings":"AAAA,MAAAA,EAAe;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;"}