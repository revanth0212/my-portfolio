{"version":3,"file":"large-reasoning-models-BSKl9S0i.js","sources":["../../src/content/blog/large-reasoning-models.md?raw"],"sourcesContent":["export default \"---\\nid: \\\"large-reasoning-models\\\"\\ntitle: \\\"Thinking Models: The Evolution from 'Autofill' to 'Architect'\\\"\\ndate: \\\"2025-12-29\\\"\\nreadTime: \\\"8 min read\\\"\\nexcerpt: \\\"Exploring the shift from standard LLMs to reasoning models that use Chain of Thought and System 2 thinking for complex problem-solving.\\\"\\ntags: [\\\"AI\\\", \\\"Machine Learning\\\", \\\"LLM\\\", \\\"Reasoning Models\\\", \\\"Chain of Thought\\\"]\\n---\\n\\nIn the early days of LLMs, we marveled at their ability to write poetry or code in seconds. However, developers quickly noticed a \\\"ceiling\\\": models were great at predicting the next word but terrible at checking their own logic. They were like highly confident interns who never double-check their work.\\n\\nEnter **Reasoning Models** (or Thinking Models). This new class of AI doesn't just predict the next token; it \\\"plans\\\" the answer before writing a single word of the final response.\\n\\n---\\n\\n## 1. System 1 vs. System 2 Thinking\\n\\nTo understand the difference, we use a framework from psychology (Daniel Kahneman):\\n\\n* **System 1 (Standard LLMs):** Fast, instinctive, and emotional. Itâ€™s like your brain instantly knowing . Most LLMs operate hereâ€”they use \\\"pattern matching\\\" to guess the answer.\\n* **System 2 (Thinking Models):** Slower, more deliberative, and logical. Itâ€™s like your brain solving . You have to stop, visualize the steps, and carry the numbers.\\n\\n### Why can't all models be \\\"System 2\\\"?\\n\\n1. **Inference Cost:** Thinking models use \\\"Chain of Thought\\\" (CoT) tokens. For every 10 words you see, the model might have written 500 words to itself. This costs significant GPU power.\\n2. **Latency:** Users don't want to wait 30 seconds for a \\\"Thinking...\\\" bubble just to get a summary of a meeting.\\n3. **The Training Wall:** You can't just tell a model to \\\"think.\\\" You have to train it using **Reinforcement Learning (RL)**, rewarding it when its internal chain of logic leads to a verifiable truth.\\n\\n---\\n\\n## 2. Built-in Reasoning vs. Agentic Wrappers\\n\\nAs a developer, youâ€™ll encounter \\\"Reasoning\\\" in two ways:\\n\\n### A. The \\\"Built-in\\\" Model (e.g., OpenAI o1, DeepSeek-R1)\\n\\nThe reasoning is part of the model's weights. During training, the model discovered that by \\\"talking to itself\\\" in a hidden scratchpad, it gets a higher reward.\\n\\n* **Pros:** Highly coherent, handles complex math/coding perfectly.\\n* **Cons:** You can't see the \\\"thought process\\\" in some proprietary models; it's a \\\"black box.\\\"\\n\\n### B. The Agentic Wrapper (The \\\"Feature\\\" approach)\\n\\nYou can force a standard model (like GPT-4o-mini) to \\\"think\\\" by using a Python loop or a framework like LangChain.\\n\\n```bash\\n# Example logic for an Agentic \\\"Thought\\\" Loop\\n1. User asks a question.\\n2. Prompt: \\\"Think step-by-step. Break the problem down.\\\"\\n3. Model outputs a 'Plan'.\\n4. Logic check: \\\"Does the plan make sense?\\\"\\n5. Model outputs final answer.\\n\\n```\\n\\n* **Pros:** Cheaper; you have total control over the \\\"thoughts.\\\"\\n* **Cons:** Usually less \\\"intelligent\\\" than a model trained specifically to reason.\\n\\n---\\n\\n## 3. The Logic Test: \\\"Sally's Brothers\\\"\\n\\nLetâ€™s see how these two architectures handle a trick question that requires spatial and relational logic.\\n\\n**The Puzzle:** *Sally has 3 brothers. Each of her brothers has 2 sisters. How many sisters does Sally have?*\\n\\n### Standard Model (The \\\"Reflex\\\" Answer) âŒ\\n\\n* **Internal logic:** \\\"I see 3 brothers and 2 sisters. ? Or maybe just 2?\\\"\\n* **Common Hallucination:** \\\"Sally has 2 sisters.\\\" (It repeats the number \\\"2\\\" because it doesn't realize Sally herself is one of the sisters).\\n\\n### Thinking Model (The \\\"Reasoned\\\" Answer) âœ…\\n\\nThe model uses an internal \\\"scratchpad\\\" (Chain of Thought):\\n\\n> **Thought:** > 1. Let's list the siblings. There are 3 brothers: B1, B2, B3.\\n> 2. Each brother has 2 sisters.\\n> 3. These 2 sisters must be the same girls for all brothers (since they are all siblings).\\n> 4. One of these sisters is Sally herself.\\n> 5. If there are 2 sisters total, and one is Sally, then there is 1 other sister.\\n> 6. Let's verify: If there is Sally and 1 sister (Jane), then B1 has two sisters (Sally and Jane). This matches the prompt.\\n\\n**Final Answer:** Sally has **1** sister.\\n\\n---\\n\\n## Executive Summary ðŸ“‹\\n\\n* **Thinking Models** are designed for high-reasoning tasks (coding, math, logic) rather than creative writing or chat.\\n* **Test-Time Compute:** These models use more \\\"brainpower\\\" *while* they are answering, rather than just relying on what they learned during training.\\n* **Native vs. Agent:** Native reasoning is trained into the model via RL; Agentic reasoning is a developer-built workflow that mimics thinking.\\n* **Key Takeaway:** If your app needs to solve a riddle or debug a complex kernel error, use a **Thinking Model**. If your app needs to draft a friendly \\\"Hello\\\" email, stick to a **Standard Model** to save money and time.\\n\""],"names":["largeReasoningModels"],"mappings":"AAAA,MAAAA,EAAe;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;"}