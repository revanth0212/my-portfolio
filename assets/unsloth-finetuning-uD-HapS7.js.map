{"version":3,"file":"unsloth-finetuning-uD-HapS7.js","sources":["../../src/content/blog/unsloth-finetuning.md?raw"],"sourcesContent":["export default \"---\\nid: \\\"unsloth-finetuning\\\"\\ntitle: \\\"A Guide to Fine-Tuning Gemma 3 using Unsloth\\\"\\ndate: \\\"2025-12-29\\\"\\nreadTime: \\\"10 min read\\\"\\nexcerpt: \\\"A technical look at how developers use Unsloth to fine-tune Google's Gemma 2B model on the cloud and deploy it locally to Apple Silicon.\\\"\\ntags: [\\\"AI\\\", \\\"Fine-tuning\\\", \\\"Unsloth\\\"]\\n---\\n\\n## ‚ú® What is Unsloth?\\n\\nIn the current AI landscape, **Unsloth** has emerged as a critical framework for fine-tuning Large Language Models (LLMs). Written in **Python**, it serves as a high-level interface for **Triton kernels**‚Äîspecialized GPU instructions that handle the heavy mathematical lifting of AI training.\\n\\nBy using \\\"kernel fusion,\\\" Unsloth combines multiple calculation steps into one, allowing models like **Gemma 2B** to train **2x to 5x faster** while consuming **70% less memory**. This efficiency makes it possible to perform professional-grade fine-tuning on free cloud hardware like Google Colab's T4 GPUs. üöÄ\\n\\n---\\n\\n## üéØ The Goal: What is Being Fine-Tuned?\\n\\nFine-tuning is the process of taking a \\\"base\\\" model‚Äîwhich has general knowledge but might be clumsy at specific tasks‚Äîand giving it specialized training. In this workflow, the model being used is **Gemma 2B**, a lightweight but powerful model from Google.\\n\\nThe \\\"lessons\\\" come from the **Alpaca Cleaned** dataset. This is a collection of 52,000 high-quality instructions (e.g., \\\"Explain quantum physics to a five-year-old\\\") paired with the ideal responses. By fine-tuning on this data, the model transforms from a general text-predictor into a **highly capable instruction-following assistant**. üß†üí°\\n\\n---\\n\\n## üõ†Ô∏è Step 1: Setting Up the Cloud Environment\\n\\nTraining begins on **Google Colab** to leverage NVIDIA's hardware. The following bash commands prepare the environment:\\n\\n```bash\\n# Installing Unsloth and essential training libraries\\npip install \\\"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\\\"\\npip install --no-deps \\\"xformers<0.0.27\\\" \\\"trl<0.9.0\\\" peft accelerate bitsandbytes\\n\\n```\\n\\n---\\n\\n## üìä Step 2: Loading the Model and Data\\n\\nDevelopers use Unsloth to load Gemma 2B in **4-bit quantization**. This \\\"squeezes\\\" the model so it fits comfortably in the GPU's memory without losing accuracy.\\n\\n```python\\nfrom unsloth import FastLanguageModel\\nimport torch\\nfrom datasets import load_dataset\\n\\n# Load the Gemma 2B model\\nmodel, tokenizer = FastLanguageModel.from_pretrained(\\n    model_name = \\\"unsloth/gemma-2-2b\\\",\\n    max_seq_length = 2048,\\n    load_in_4bit = True,\\n)\\n\\n# Load the Alpaca Cleaned dataset\\ndataset = load_dataset(\\\"yahma/alpaca-cleaned\\\", split = \\\"train\\\")\\n\\n```\\n\\nThe data must be formatted into a structure the model understands. This is done using an **Alpaca Prompt Template**:\\n\\n```python\\nalpaca_prompt = \\\"\\\"\\\"Below is an instruction that describes a task. \\nWrite a response that appropriately completes the request.\\n\\n### Instruction:\\n{}\\n\\n### Response:\\n{}\\\"\\\"\\\"\\n\\ndef formatting_prompts_func(examples):\\n    instructions = examples[\\\"instruction\\\"]\\n    outputs      = examples[\\\"output\\\"]\\n    texts = []\\n    for instruction, output in zip(instructions, outputs):\\n        text = alpaca_prompt.format(instruction, output) + tokenizer.eos_token\\n        texts.append(text)\\n    return { \\\"text\\\" : texts }\\n\\ndataset = dataset.map(formatting_prompts_func, batched = True)\\n\\n```\\n\\n---\\n\\n## üî• Step 3: The Fine-Tuning Process\\n\\nOnce the data is ready, the training loop begins. Unsloth uses **LoRA (Low-Rank Adaptation)**, which only updates a tiny fraction of the model's weights, making the process incredibly fast.\\n\\n```python\\nfrom trl import SFTTrainer\\nfrom transformers import TrainingArguments\\n\\ntrainer = SFTTrainer(\\n    model = model,\\n    train_dataset = dataset,\\n    dataset_text_field = \\\"text\\\",\\n    max_seq_length = 2048,\\n    args = TrainingArguments(\\n        per_device_train_batch_size = 2,\\n        gradient_accumulation_steps = 4,\\n        warmup_steps = 5,\\n        max_steps = 60, # Small step count for demonstration\\n        learning_rate = 2e-4,\\n        fp16 = True,\\n        logging_steps = 1,\\n        output_dir = \\\"outputs\\\",\\n    ),\\n)\\ntrainer.train()\\n\\n```\\n\\n---\\n\\n## üì¶ Step 4: Exporting for the Mac (GGUF)\\n\\nAfter the AI has finished its \\\"lessons,\\\" it must be converted into a format that Apple Silicon (M1/M2/M3) can run. This format is called **GGUF**.\\n\\n```python\\n# Save the model in GGUF format for local use\\nmodel.save_pretrained_gguf(\\\"gemma_instruction_model.gguf\\\", tokenizer, quantization_method = \\\"q4_k_m\\\")\\n\\n```\\n\\n---\\n\\n## üè† Step 5: Local Deployment on Mac\\n\\nThe final `.gguf` file is downloaded from Colab to the user's **MacBook Pro**. To run the model, developers typically use **LM Studio**.\\n\\n1. **Open LM Studio** on the Mac.\\n2. **Import** the `gemma_instruction_model.gguf` file.\\n3. The model now runs entirely locally, using the Mac's **Unified Memory** for lightning-fast responses without needing an internet connection. üíªüçé\\n\\nBy following this cloud-to-local workflow, developers can build specialized AI tools that are private, fast, and custom-tailored to their specific needs.\\n\\n---\\n\\n## üèÅ Summary: The Unsloth Advantage\\n\\nThe workflow of fine-tuning an LLM has been revolutionized by the synergy between cloud computing and local inference. By leveraging **Unsloth**, developers can bypass the need for expensive local hardware during the intensive training phase, while still enjoying the privacy and speed of local execution on **Apple Silicon**.\\n\\n### üóùÔ∏è Key Takeaways\\n\\n* **Efficiency:** Unsloth uses **Triton kernels** to make fine-tuning **2x-5x faster** with **70% less VRAM**, enabling the use of free tools like **Google Colab**. ‚ö°\\n* **Specialization:** By using datasets like **Alpaca Cleaned**, a base model like **Gemma 2B** is transformed into a specialized instruction-following assistant. üß†\\n* **Portability:** The conversion to **GGUF** acts as a bridge, allowing complex models to be compressed and run on consumer-grade hardware. üåâ\\n* **Local Control:** Tools like **LM Studio** provide a user-friendly interface to run these custom models on a **MacBook Pro**, ensuring data privacy and zero latency. üíªüçé\\n\\nThis hybrid approach represents the future of accessible AI development‚Äîputting the power of customized large language models directly into the hands of individual developers and researchers. üåü\\n\""],"names":["unslothFinetuning"],"mappings":"AAAA,MAAAA,EAAe;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;"}